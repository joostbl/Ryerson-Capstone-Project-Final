---
title: "Sentiment Analysis FINAL"
author: "Joost Bloos"
date: "07/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Install packages:
#Source: https://trinkerrstuff.wordpress.com/my-r-packages/qdap/

#if (!require("pacman")) install.packages("pacman")
#pacman::p_load(sentimentr, dplyr, magrittr)
#install.packages("devtools")
#install_github("trinker/qdapDictionaries")
#install_github("trinker/qdapRegex")
#install_github("trinker/qdapTools")
#install_github("trinker/qdap")
#install.packages("quanteda")
#install.packages("sentimentr")
#install.packages("ndjson")
#install.packages("NLP")
#install.packages("dplyr")
#install.packages("tidyr")
#install.packages("tm")
#install.packages("corpus")
#install.packages("syuzhet")
#install.packages("plotly")
#install.packages("wordcloud")
#install.packages("profmem")

```


```{r}
library(devtools)
library(tm)
library(qdap)
library(sentimentr)
library(ndjson)
library(corpus)
library(syuzhet)
library(tidyr)
library(dplyr)
library(quanteda)
library(ggplot2)
library(plotly)
library(wordcloud)
library(profmem)

#a good package, also takes into account  negative words and amplifiers
#see: http://www.inside-r.org/packages/cran/qdap/docs/polarity
```

```{r}
#getwd()
#setwd("C:/Ryerson University - Capstone project/Module 2/EIEEE - Large dataset/Combined")
```

```{r}
rawData100K <- read.csv(file = "rawData100K.csv")

str(rawData100K)
```

```{r}
#create a corpus:
importdocs = corpus(rawData100K, text_field = 'text')
```

```{r}
#preprocessing of data
importdocs <- gsub("'", "", importdocs)  # remove apostrophes
importdocs <- gsub("[[:punct:]]", " ", importdocs)  # replace punctuation with space
importdocs <- gsub("[[:cntrl:]]", " ", importdocs)  # replace control characters with space
importdocs <- gsub("^[[:space:]]+", "", importdocs) # remove whitespace at beginning of documents
importdocs <- gsub("[[:space:]]+$", "", importdocs) # remove whitespace at end of documents
importdocs <- tolower(importdocs)

# CLEANING TWEETS
importdocs=gsub("&amp", "", importdocs)
importdocs = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", importdocs)
importdocs = gsub("@\\w+", "", importdocs)
importdocs = gsub("[[:digit:]]", "", importdocs)
importdocs = gsub("http\\w+", "", importdocs)
importdocs = gsub("[ \t]{2,}", "", importdocs)
importdocs = gsub("^\\s+|\\s+$", "", importdocs)
importdocs <- iconv(importdocs, "UTF-8", "ASCII", sub="")

str(importdocs)
```


```{r}
#Create corpus with get-sentences function:
mycorpus <- get_sentences(importdocs) # creates a character vector and tokenize by sentence

head(mycorpus,10)
```



```{r}
#apply Syuzhet package, source : https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html
#Measure running time

start_time <- Sys.time()
mysentiment <- get_sentiment(mycorpus, method= "afinn")
end_time <- Sys.time()

end_time - start_time


```


```{r}

start_time <- Sys.time()
mysentiment_memory <- profmem({
  get_sentiment(mycorpus, method= "afinn")
})
end_time <- Sys.time()

end_time - start_time
#print(mysentiment_memory, expr = FALSE)

#Giga Bytes used:
sum((mysentiment_memory$bytes), na.rm=TRUE)/1000000000 

```

```{r}
mysentiment <- get_sentiment(mycorpus, method= "afinn")

str(mysentiment)
```

```{r}
# run overall score, result overall neutral to perhaps moderate positive
summary(mysentiment)
```

```{r}
#results expressed in histogram

qplot(mysentiment,   geom="histogram",binwidth=0.1,main="Review Sentiment Histogram - (afinn)")
```



```{r}
#integrate sentiment score into updated dataset
#sentimentResultMay2020 <- rawData100K

#sentimentResultMay2020$sentiment_afinn = mysentiment
#str(sentimentResultMay2020)
```

```{r}
#write sentiment score to original dataset

#write.csv(sentimentResultMay2020,'sentimentResultMay2020.csv')
```


